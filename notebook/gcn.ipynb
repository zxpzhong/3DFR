{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. V1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GraphProjection(nn.Module):\n",
    "    \"\"\"Graph Projection layer, which pool 2D features to mesh\n",
    "    The layer projects a vertex of the mesh to the 2D image and use \n",
    "    bilinear interpolation to get the corresponding feature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphProjection, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, img_features, input):\n",
    "\n",
    "        self.img_feats = img_features \n",
    "        # 决定图像宽高\n",
    "        # h = 248 * x/z + 111.5\n",
    "        # w = 248 * y/z + 111.5\n",
    "        h = 248 * torch.div(input[:, 1], input[:, 2]) + 111.5\n",
    "        w = 248 * torch.div(input[:, 0], -input[:, 2]) + 111.5\n",
    "        # 裁剪图像，最大值为223 （即图像为<=224）\n",
    "        h = torch.clamp(h, min = 0, max = 223)\n",
    "        w = torch.clamp(w, min = 0, max = 223)\n",
    "        # 特征图尺寸\n",
    "        img_sizes = [56, 28, 14, 7]\n",
    "        out_dims = [64, 128, 256, 512]\n",
    "        feats = [input]\n",
    "        # 四次投影\n",
    "        for i in range(4):\n",
    "            out = self.project(i, h, w, img_sizes[i], out_dims[i])\n",
    "            feats.append(out)\n",
    "        # 四次投影的特征直接cat\n",
    "        output = torch.cat(feats, 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def project(self, index, h, w, img_size, out_dim):\n",
    "        # 第index次投影， 图像尺寸h*w , 图像尺寸img_size（xy方向相同）\n",
    "        \n",
    "        # 取出本次特征\n",
    "        img_feat = self.img_feats[index]\n",
    "        # 计算出图像尺寸大小和224原图的相对百分比，由此得出输出特征图尺寸相对于当前特征图大小\n",
    "        x = h / (224. / img_size)\n",
    "        y = w / (224. / img_size)\n",
    "        # torch.floor(x) ： 小于等于x的最大整数\n",
    "        # torch.ceil(x) ： 大于等于x的最小整数\n",
    "        x1, x2 = torch.floor(x).long(), torch.ceil(x).long()\n",
    "        y1, y2 = torch.floor(y).long(), torch.ceil(y).long()\n",
    "        # 按图像尺寸阶段最大值\n",
    "        x2 = torch.clamp(x2, max = img_size - 1)\n",
    "        y2 = torch.clamp(y2, max = img_size - 1)\n",
    "\n",
    "        #Q11 = torch.index_select(torch.index_select(img_feat, 1, x1), 1, y1)\n",
    "        #Q12 = torch.index_select(torch.index_select(img_feat, 1, x1), 1, y2)\n",
    "        #Q21 = torch.index_select(torch.index_select(img_feat, 1, x2), 1, y1)\n",
    "        #Q22 = torch.index_select(torch.index_select(img_feat, 1, x2), 1, y2)\n",
    "\n",
    "        # Q11为\n",
    "        Q11 = img_feat[:, x1, y1].clone()\n",
    "        Q12 = img_feat[:, x1, y2].clone()\n",
    "        Q21 = img_feat[:, x2, y1].clone()\n",
    "        Q22 = img_feat[:, x2, y2].clone()\n",
    "\n",
    "        x, y = x.long(), y.long()\n",
    "        # 双线性插值\n",
    "        weights = torch.mul(x2 - x, y2 - y)\n",
    "        \n",
    "        Q11 = torch.mul(weights.float().view(-1, 1), torch.transpose(Q11, 0, 1))\n",
    "\n",
    "        weights = torch.mul(x2 - x, y - y1)\n",
    "        Q12 = torch.mul(weights.float().view(-1, 1), torch.transpose(Q12, 0 ,1))\n",
    "\n",
    "        weights = torch.mul(x - x1, y2 - y)\n",
    "        Q21 = torch.mul(weights.float().view(-1, 1), torch.transpose(Q21, 0, 1))\n",
    "\n",
    "        weights = torch.mul(x - x1, y - y1)\n",
    "        Q22 = torch.mul(weights.float().view(-1, 1), torch.transpose(Q22, 0, 1))\n",
    "\n",
    "        output = Q11 + Q21 + Q12 + Q22\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GraphProjection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "blocks = 4\n",
    "channels = 16\n",
    "h = [56, 28, 14, 7]\n",
    "w = [56, 28, 14, 7]\n",
    "img_features = []\n",
    "for i in range(4):\n",
    "    img_features.append (torch.rand((bs,h[i],w[i])))\n",
    "\n",
    "N = 500\n",
    "dim = 3\n",
    "input = torch.rand((bs,N,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (4) at non-singleton dimension 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b761c76c15c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/software/anaconda3/envs/kaolin/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8dbaf99de799>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img_features, input)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 四次投影\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# 四次投影的特征直接cat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8dbaf99de799>\u001b[0m in \u001b[0;36mproject\u001b[0;34m(self, index, h, w, img_size, out_dim)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mQ11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "gp(img_features,input)"
   ]
  },
  {
   "source": [
    "# 2. V2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Projection layer, which pool 2D features to mesh\n",
    "\n",
    "    The layer projects a vertex of the mesh to the 2D image and use\n",
    "    bi-linear interpolation to get the corresponding feature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mesh_pos, camera_f, camera_c, bound=0, tensorflow_compatible=False):\n",
    "        super(GProjection, self).__init__()\n",
    "        self.mesh_pos, self.camera_f, self.camera_c = mesh_pos, camera_f, camera_c\n",
    "        self.threshold = None\n",
    "        self.bound = 0\n",
    "        self.tensorflow_compatible = tensorflow_compatible\n",
    "        if self.bound != 0:\n",
    "            self.threshold = Threshold(bound, bound)\n",
    "\n",
    "    def bound_val(self, x):\n",
    "        \"\"\"\n",
    "        given x, return min(threshold, x), in case threshold is not None\n",
    "        \"\"\"\n",
    "        if self.bound < 0:\n",
    "            return -self.threshold(-x)\n",
    "        elif self.bound > 0:\n",
    "            return self.threshold(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def image_feature_shape(img):\n",
    "        return np.array([img.size(-1), img.size(-2)])\n",
    "\n",
    "    def project_tensorflow(self, x, y, img_size, img_feat):\n",
    "        x = torch.clamp(x, min=0, max=img_size[1] - 1)\n",
    "        y = torch.clamp(y, min=0, max=img_size[0] - 1)\n",
    "\n",
    "        # it's tedious and contains bugs...\n",
    "        # when x1 = x2, the area is 0, therefore it won't be processed\n",
    "        # keep it here to align with tensorflow version\n",
    "        x1, x2 = torch.floor(x).long(), torch.ceil(x).long()\n",
    "        y1, y2 = torch.floor(y).long(), torch.ceil(y).long()\n",
    "\n",
    "        Q11 = img_feat[:, x1, y1].clone()\n",
    "        Q12 = img_feat[:, x1, y2].clone()\n",
    "        Q21 = img_feat[:, x2, y1].clone()\n",
    "        Q22 = img_feat[:, x2, y2].clone()\n",
    "\n",
    "        weights = torch.mul(x2.float() - x, y2.float() - y)\n",
    "        Q11 = torch.mul(weights.unsqueeze(-1), torch.transpose(Q11, 0, 1))\n",
    "\n",
    "        weights = torch.mul(x2.float() - x, y - y1.float())\n",
    "        Q12 = torch.mul(weights.unsqueeze(-1), torch.transpose(Q12, 0, 1))\n",
    "\n",
    "        weights = torch.mul(x - x1.float(), y2.float() - y)\n",
    "        Q21 = torch.mul(weights.unsqueeze(-1), torch.transpose(Q21, 0, 1))\n",
    "\n",
    "        weights = torch.mul(x - x1.float(), y - y1.float())\n",
    "        Q22 = torch.mul(weights.unsqueeze(-1), torch.transpose(Q22, 0, 1))\n",
    "\n",
    "        output = Q11 + Q21 + Q12 + Q22\n",
    "        return output\n",
    "\n",
    "    def forward(self, resolution, img_features, inputs):\n",
    "        half_resolution = (resolution - 1) / 2\n",
    "        camera_c_offset = np.array(self.camera_c) - half_resolution\n",
    "        # map to [-1, 1]\n",
    "        # not sure why they render to negative x\n",
    "        positions = inputs + torch.tensor(self.mesh_pos, device=inputs.device, dtype=torch.float)\n",
    "        w = -self.camera_f[0] * (positions[:, :, 0] / self.bound_val(positions[:, :, 2])) + camera_c_offset[0]\n",
    "        h = self.camera_f[1] * (positions[:, :, 1] / self.bound_val(positions[:, :, 2])) + camera_c_offset[1]\n",
    "\n",
    "        if self.tensorflow_compatible:\n",
    "            # to align with tensorflow\n",
    "            # this is incorrect, I believe\n",
    "            w += half_resolution[0]\n",
    "            h += half_resolution[1]\n",
    "\n",
    "        else:\n",
    "            # directly do clamping\n",
    "            w /= half_resolution[0]\n",
    "            h /= half_resolution[1]\n",
    "\n",
    "            # clamp to [-1, 1]\n",
    "            w = torch.clamp(w, min=-1, max=1)\n",
    "            h = torch.clamp(h, min=-1, max=1)\n",
    "\n",
    "        feats = [inputs]\n",
    "        for img_feature in img_features:\n",
    "            feats.append(self.project(resolution, img_feature, torch.stack([w, h], dim=-1)))\n",
    "\n",
    "        output = torch.cat(feats, 2)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def project(self, img_shape, img_feat, sample_points):\n",
    "        \"\"\"\n",
    "        :param img_shape: raw image shape\n",
    "        :param img_feat: [batch_size x channel x h x w]\n",
    "        :param sample_points: [batch_size x num_points x 2], in range [-1, 1]\n",
    "        :return: [batch_size x num_points x feat_dim]\n",
    "        \"\"\"\n",
    "        if self.tensorflow_compatible:\n",
    "            feature_shape = self.image_feature_shape(img_feat)\n",
    "            points_w = sample_points[:, :, 0] / (img_shape[0] / feature_shape[0])\n",
    "            points_h = sample_points[:, :, 1] / (img_shape[1] / feature_shape[1])\n",
    "            output = torch.stack([self.project_tensorflow(points_h[i], points_w[i],\n",
    "                                                          feature_shape, img_feat[i]) for i in range(img_feat.size(0))], 0)\n",
    "        else:\n",
    "            output = F.grid_sample(img_feat, sample_points.unsqueeze(1))\n",
    "            output = torch.transpose(output.squeeze(2), 1, 2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = GProjection(mesh_pos = [0,0,0], camera_f = [483.76885985,483.969696], camera_c = [351.56368,175.50937], bound=0, tensorflow_compatible=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "blocks = 4\n",
    "channels = 16\n",
    "h = [56, 28, 14, 7]\n",
    "w = [56, 28, 14, 7]\n",
    "img_features = []\n",
    "for i in range(4):\n",
    "    img_features.append (torch.rand((bs,channels,h[i],w[i])))\n",
    "\n",
    "N = 500\n",
    "dim = 3\n",
    "input = torch.rand((bs,N,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolution, img_features, inputs\n",
    "output = pro(np.array([224,224]),img_features,input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 500, 67])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}